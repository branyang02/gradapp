\documentclass[12pt]{article}
\usepackage[
  a4paper,
  margin=1in,
  headsep=18pt, % separation between header rule and text
]{geometry}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{microtype}
\usepackage[backend=biber,style=numeric]{biblatex}
\usepackage{setspace}
% \usepackage{mathpazo}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{fontawesome}


\definecolor{customred}{HTML}{d14836}


\addbibresource{references.bib}

\singlespacing
% \setstretch{1.5}
\setlength{\parskip}{1em} 

\newcommand{\statement}[1]{\medskip\noindent
  \textcolor{black}{\textbf{#1}}\space
}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{
\textsc{\soptitle}\hfill\footnotesize
\textbf{\yourname}\\
\school\hfill\yourintent}
\fancyfoot[C]{\footnotesize\thepage}

\let\oldcenter\center
\let\oldendcenter\endcenter
\renewenvironment{center}{\setlength\topsep{0pt}\oldcenter}{\oldendcenter}

\newcommand{\wordcount}{
\begin{center}
\rule{0.75\textwidth}{.4pt}\\
{\footnotesize A statement of \numwords \ words}
\end{center}
}

\newif\ifcomments

\newcommand{\comment}[1]{}
\newcommand{\todo}[1]{\ifcomments\textcolor{red}{TODO: #1}\fi}

%%%%%%%%%%%%%%%%%%%%% Edit this section %%%%%%%%%%%%%%%%%%%%

\commentstrue

\newcommand{\soptitle}{Statement of Purpose}
\newcommand{\yourname}{Brandon Y. Yang}
\newcommand{\yourintent}{\faLink \, \href{https://brandonyifanyang.com/}{brandonyifanyang.com} \quad \faEnvelope \, \href{mailto:branyang@virginia.edu}{branyang@virginia.edu}}

\newcommand{\school}{Emory University}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[
  breaklinks,
  pdftitle={\yourname - \soptitle},
  pdfauthor={\yourname},
  unicode,
  colorlinks,
]{hyperref}

\begin{document}

\noindent In the modern era of embodied Artificial Intelligence (AI) systems, the integration of Natural Language Processing (NLP) and Computer Vision (CV) foundational models trained on extensive data has propelled efforts to develop generalist robotic policies. Despite these efforts, current models often fall short in \textbf{reasoning}, \textbf{planning}, and \textbf{executing} \textit{long-horizon} tasks. My research focuses on addressing these challenges at the intersection of NLP and robotics, where I aim to:
\begin{enumerate}[label=(\arabic*), itemindent=0pt, itemsep=0pt, parsep=0pt, nosep]
    \item \textbf{Develop embodied agents that follow and interact with natural language \textit{collaboratively}.}
    \item \textbf{Leverage language for reasoning and planning to navigate complex environments and solve long-horizon tasks.}
\end{enumerate}

\statement{Reinforcement Learning and the Need for Reasoning} Robotics is inherently collaborative, yet enabling true \textit{autonomous} cooperation in \textit{multi-agent} systems remains challenging. To pursue my interest, I joined the \href{https://www.collabrobotics.com/}{Collaborative Robotics Lab} at the \textbf{University of Virginia} (UVA) with \href{https://www.tiqbal.com/}{Prof. Tariq Iqbal}, focusing on multi-agent RL to develop collaborative policies. I developed simulation environments for complex assembly tasks and designed offline centralized RL policies. However, I realized our robots' successes heavily depended on meticulously crafted reward functions, requiring labor-intensive design processes. This reliance highlighted a significant limitation: \textit{the inability of robots to adapt to new environments without extensive human input}. Observing this, I became curious about whether robots could mimic how humans navigate complex environments through innate reasoning rather than external supervision.

\statement{Language and Vision Guided Robotic Manipulation} Motivated by the limitations of reward supervision, I explored \textit{language} and \textit{vision}-driven approaches to enable autonomous reasoning and action. I developed \textit{GLOMA: Grounded Location for Object Manipulation}, a framework using large language models (LLM) and image diffusion models to generate goal images from language instructions. \textit{GLOMA} enables robots to execute goal-conditioned policies without the need for manually crafted reward functions, allowing them to autonomously imagine subgoals for long-horizon tasks. I led the development of \textit{GLOMA}, including dataset creation and model fine-tuning, and presented it at multiple research symposiums. However, I realized that traditional 2D image-based methods fail to capture the 3D semantics and object relationships of natural environments, motivating me to explore 3D-based methods for fine-grained robotic perception.

\statement{Advancing Robotic Perception with 3D Gaussian Splatting} To continue goal synthesis motivated by autonomous policies, I expanded to 3D by collaborating with \href{https://jbhuang0604.github.io/}{Prof. Jia-Bin Huang} at the \textbf{University of Maryland} and colleagues from \textbf{MIT}. Our research addresses the limitations of 2D synthesis in environments needing 3D understanding, such as vertical displacement. We leverage 3D Gaussian Splatting (3DGS) for highly accurate 3D field representation, enabling more effective robotic perception. To enhance semantic understanding, we inject embeddings from large 2D foundational models into 3DGS, allowing robots to comprehend scene semantics and perform object-level edits. Our preliminary results demonstrate that 3D goal synthesis enables more robust and precise robotic manipulation. As project lead, I developed the codebase and explored various embedding injection techniques.

\statement{Enhancing Robotic Planning with Skill-Conditioned Architectures} While goal-synthesis methods in both 2D and 3D can be robust and interpretable, they incur significant computational overhead due to their multi-step processes. Recent research has focused on developing end-to-end Vision-Language-Action (VLA) models to streamline this process. However, these models lack interpretability and generalization, especially with out-of-distribution data. To overcome these limitations, I am collaborating with \href{https://yenlingkuo.com/}{Prof. Yen-Ling Kuo} at \textbf{UVA} to develop \textit{SkillVLA}, a novel VLA model that enhances long-horizon, language-guided robotic policies by introducing a \textit{skill-conditioned} action output space. In \textit{SkillVLA}, each action is grounded to a specific skill—such as \textit{grasp} or \textit{lift}—which improves the interpretability and robustness of the policy. This approach enables robots to perform complex tasks more efficiently and adapt to diverse environments. I plan to submit this work to \textbf{RSS 2025} and believe \textit{SkillVLA} can advance skill-based learning in modern robotic manipulations.

\statement{Future Plans} I plan to extend my research in skill-conditioned reasoning and planning for embodied agents by developing systems that utilize \textbf{complex semantic concepts} (e.g., \textit{object affordances, spatial relations, grasping strategies}) to enhance their understanding of the physical world. Acquiring these concepts alongside skill manipulation priors (e.g., \textit{picking, placing}) enables embodied agents to \textbf{reason}, \textbf{plan}, and \textbf{execute} actions based on a deeper understanding of objects and scenes. This approach could facilitate \textbf{robust generalist robotic policies} for language-guided manipulation in complex environments. I will leverage insights from NLP and multimodal communities, which have made significant progress in modeling semantic structures across perceptual inputs. Furthermore, I believe richer 3D environment representations can enhance concept learning, allowing agents more informed reasoning about the physical world.

\noindent I also aim to develop embodied agents that can work \textbf{\textit{collaboratively}}. While modern LLMs can converse with humans, current robot systems have yet to fully leverage this capability for collaboration in physical tasks. By integrating NLP methods for collaborative dialogue into embodied AI, I aim to enable robots to perform \textit{collaborative} tasks, such as assembling furniture or cleaning, in partnership with humans or other agents.

\statement{Why Emory} Emory University's Ph.D. program in Computer Science aligns with my academic and research goals, particularly in the areas of NLP and embodied AI. I am excited about the opportunity to work with \href{https://cs.emory.edu/~lzhao41/}{\textbf{Prof. Liang Zhao}}, whose research in multimodal explanation-guided learning and reasoning closely aligns with my past experiences. In addition, I am interested in his research in structure-guided representation learning, as I believe that learning and understanding in the latent space can significantly improve robustness and interpretability of robot policies. The work of \href{https://www.cs.emory.edu/~fliu40/index.html}{\textbf{Prof. Fei Liu}} in reasoning and decision-making in NLP systems would help me develop more sophisticated embodied agents that can reason and plan autonomously. Specifically, I am drawn to her recent work such as \textit{STRUX}, \textit{DeFine}, and \textit{SportsMetrics}, and I want to explore how these NLP methods can be adapted to robot decision-making. Furthermore, I look forward to collaborating with \href{https://www.cs.emory.edu/~kshu5/}{\textbf{Prof. Kai Shu}}, whose research in multimodal learning, factuality, and reliability is crucial for developing trustworthy and deployable robot systems. I aim to extend his current methods in model explainability to robot systems, ensuring that robots can seamlessly work with humans through predictable and interpretable actions.

\end{document}