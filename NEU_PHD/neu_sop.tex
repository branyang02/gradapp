\documentclass[12pt]{article}
\usepackage[
  a4paper,
  margin=1in,
  headsep=18pt, % separation between header rule and text
]{geometry}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{microtype}
\usepackage[backend=biber,style=numeric]{biblatex}
\usepackage{setspace}
% \usepackage{mathpazo}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{fontawesome}


\definecolor{customred}{HTML}{d14836}


\addbibresource{references.bib}

\singlespacing
% \setstretch{1.5}
\setlength{\parskip}{1em} 

\newcommand{\statement}[1]{\medskip\noindent
  \textcolor{black}{\textbf{#1}}\space
}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{
\textsc{\soptitle}\hfill\footnotesize
\textbf{\yourname}\\
\school\hfill\yourintent}
\fancyfoot[C]{\footnotesize\thepage}

\let\oldcenter\center
\let\oldendcenter\endcenter
\renewenvironment{center}{\setlength\topsep{0pt}\oldcenter}{\oldendcenter}

\newcommand{\wordcount}{
\begin{center}
\rule{0.75\textwidth}{.4pt}\\
{\footnotesize A statement of \numwords \ words}
\end{center}
}

\newif\ifcomments

\newcommand{\comment}[1]{}
\newcommand{\todo}[1]{\ifcomments\textcolor{red}{TODO: #1}\fi}

%%%%%%%%%%%%%%%%%%%%% Edit this section %%%%%%%%%%%%%%%%%%%%

\commentstrue

\newcommand{\soptitle}{Statement of Purpose}
\newcommand{\yourname}{Brandon Y. Yang}
\newcommand{\yourintent}{\faLink \, \href{https://brandonyifanyang.com/}{brandonyifanyang.com} \quad \faEnvelope \, \href{mailto:branyang@virginia.edu}{branyang@virginia.edu}}

\newcommand{\school}{Northeastern University}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[
  breaklinks,
  pdftitle={\yourname - \soptitle},
  pdfauthor={\yourname},
  unicode,
  colorlinks,
]{hyperref}

\begin{document}

\noindent In the modern era of embodied Artificial Intelligence (AI) systems, the integration of Natural Language Processing (NLP) and Computer Vision (CV) foundational models trained on extensive data has propelled efforts to develop generalist robotic policies. Despite these efforts, current models often fall short in \textbf{reasoning}, \textbf{planning}, and \textbf{executing} \textit{long-horizon} tasks. My research focuses on addressing these challenges at the intersection of NLP and robotics, where I aim to:
\begin{enumerate}[label=(\arabic*), itemindent=0pt, itemsep=0pt, parsep=0pt, nosep]
  \item \textbf{Develop embodied agents that follow and interact with natural language \textit{collaboratively}.}
  \item \textbf{Leverage language for reasoning and planning to navigate complex environments and solve long-horizon tasks.}
\end{enumerate}

\statement{Reinforcement Learning and the Need for Reasoning} Robotics is inherently collaborative, yet enabling true \textit{autonomous} cooperation in \textit{multi-agent} systems remains challenging. To pursue my interest, I joined the \href{https://www.collabrobotics.com/}{Collaborative Robotics Lab} at the \textbf{University of Virginia} (UVA) with \href{https://www.tiqbal.com/}{Prof. Tariq Iqbal}, focusing on multi-agent RL to develop collaborative policies. I developed simulation environments for complex assembly tasks and designed offline centralized RL policies. However, I realized our robots' successes heavily depended on meticulously crafted reward functions, requiring labor-intensive design processes. This reliance highlighted a significant limitation: \textit{the inability of robots to adapt to new environments without extensive human input}. Observing this, I became curious about whether robots could mimic how humans navigate complex environments through innate reasoning rather than external supervision.

\statement{Language and Vision Guided Robotic Manipulation} Motivated by the limitations of reward supervision, I explored \textit{language} and \textit{vision}-driven approaches to enable autonomous reasoning and action. I developed \textit{GLOMA: Grounded Location for Object Manipulation}, a framework using large language models (LLM) and image diffusion models to generate goal images from language instructions. \textit{GLOMA} enables robots to execute goal-conditioned policies without manually crafted reward functions, thereby autonomously \textit{imagine} subgoals for long-horizon tasks. I led the development of \textit{GLOMA}, including dataset creation and model fine-tuning, and presented it at multiple research symposiums. However, I realized that traditional 2D image-based methods fail to capture the 3D semantics and object relationships of natural environments, motivating me to explore 3D-based methods for fine-grained robotic perception.

\statement{Advancing Robotic Perception with 3D Gaussian Splatting} To continue goal synthesis motivated by autonomous policies, I expanded to 3D by collaborating with \href{https://jbhuang0604.github.io/}{Prof. Jia-Bin Huang} at the \textbf{University of Maryland} and colleagues from \textbf{MIT}. Our research addresses the limitations of 2D synthesis in environments needing 3D understanding, such as vertical displacement. We leverage 3D Gaussian Splatting (3DGS) for highly accurate 3D field representation, enabling more effective robotic perception. To enhance semantic understanding, we inject embeddings from large 2D foundational models into 3DGS, allowing robots to comprehend scene semantics and perform object-level edits. Our preliminary results demonstrate that 3D goal synthesis enables more robust and precise robotic manipulation. As project lead, I developed the codebase and explored various embedding injection techniques.

\statement{Enhancing Robotic Planning with Skill-Conditioned Architectures} While goal-synthesis methods in both 2D and 3D can be robust and interpretable, they incur significant computational overhead due to their multi-step processes. Recent research has focused on developing end-to-end Vision-Language-Action (VLA) models to streamline this process. However, these models lack interpretability and generalization, especially with out-of-distribution data. To overcome these limitations, I am collaborating with \href{https://yenlingkuo.com/}{Prof. Yen-Ling Kuo} at \textbf{UVA} to develop \textit{SkillVLA}, a novel VLA model that enhances long-horizon, language-guided robotic policies by introducing a \textit{skill-conditioned} action output space. In \textit{SkillVLA}, each action is grounded to a specific skill—such as \textit{grasp} or \textit{lift}—which improves the interpretability and robustness of the policy. This approach enables robots to perform complex tasks more efficiently and adapt to diverse environments. I plan to submit this work to \textbf{RSS 2025} and believe \textit{SkillVLA} can advance skill-based learning in modern robotic manipulations.

\statement{Future Plans} I plan to extend my research in skill-conditioned reasoning and planning for embodied agents by developing systems that utilize \textbf{complex semantic concepts} (e.g., \textit{object affordances, spatial relations, grasping strategies}) to enhance their understanding of the physical world. Acquiring these concepts alongside skill manipulation priors (e.g., \textit{picking, placing}) enables embodied agents to \textbf{reason}, \textbf{plan}, and \textbf{execute} actions based on a deeper understanding of objects and scenes. This approach could facilitate \textbf{robust generalist robotic policies} for language-guided manipulation in complex environments. I will leverage insights from NLP and multimodal communities, which have made significant progress in modeling semantic structures across perceptual inputs. Furthermore, I believe richer 3D environment representations can enhance concept learning, allowing agents more informed reasoning about the physical world.

\noindent I also aim to develop embodied agents that can work \textbf{\textit{collaboratively}}. While modern LLMs can converse with humans, current robot systems have yet to fully leverage this capability for collaboration in physical tasks. By integrating NLP methods for collaborative dialogue into embodied AI, I aim to enable robots to perform \textit{collaborative} tasks, such as assembling furniture or cleaning, in partnership with humans or other agents.

\statement{Why Northeastern} Northeastern's Ph.D. program stands out for its pioneering research in robotics and AI. I am particularly drawn to the work of \href{https://www2.ccs.neu.edu/research/helpinghands/group.html}{\textbf{Prof. Robert Platt}}, whose focus on building robots that collaborate with humans aligns closely with my personal research goals. I am inspired by his use of generative models, in addition to language models, to achieve robot learning, as demonstrated in his work \textit{Imagination Policy}. Similarly, \href{https://zhi.fyi/}{\textbf{Prof. Zhi Tan}}'s work on human-robot interaction resonates with my interest in designing robots that specifically work with humans using language. My experience in language-guided robot learning can contribute to developing adaptive policies for collaborative agents. Finally, I am very interested in collaborating with \href{https://www.khoury.northeastern.edu/home/lsw/}{\textbf{Prof. Lawson L.S. Wong}}, who focuses on how language can be used to perform reasoning for robots, as demonstrated in his paper \textit{The wallpaper is ugly: Indoor Localization using Vision and Language}. My experience in NLP and robotics will enable me to contribute meaningfully to his projects. I am confident that my background, combined with the resources and expertise at Northeastern, will enable me to make contributions to robotics and AI research while preparing me for a career as a future professor or industry research scientist in this domain.

\end{document}