\documentclass[11pt]{article}
\usepackage[
  a4paper,
  margin=1in,
  headsep=18pt, % separation between header rule and text
]{geometry}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{microtype}
\usepackage[backend=biber,style=numeric]{biblatex}
\usepackage{setspace}
% \usepackage{mathpazo}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{fontawesome}


\definecolor{customred}{HTML}{d14836}


\addbibresource{references.bib}

\singlespacing
% \setstretch{1.5}
\setlength{\parskip}{1em} 

\newcommand{\statement}[1]{\medskip\noindent
  \textcolor{black}{\textbf{#1}}\space
}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{
\textsc{\soptitle}\hfill\footnotesize
\textbf{\yourname}\\
\school\hfill\yourintent}
\fancyfoot[C]{\footnotesize\thepage}

\let\oldcenter\center
\let\oldendcenter\endcenter
\renewenvironment{center}{\setlength\topsep{0pt}\oldcenter}{\oldendcenter}

\newcommand{\wordcount}{
\begin{center}
\rule{0.75\textwidth}{.4pt}\\
{\footnotesize A statement of \numwords \ words}
\end{center}
}

\newif\ifcomments

\newcommand{\comment}[1]{}
\newcommand{\todo}[1]{\ifcomments\textcolor{red}{TODO: #1}\fi}

%%%%%%%%%%%%%%%%%%%%% Edit this section %%%%%%%%%%%%%%%%%%%%

\commentstrue

\newcommand{\soptitle}{Academic Statement}
\newcommand{\yourname}{Brandon Y. Yang}
\newcommand{\yourintent}{\faLink \, \href{https://brandonyifanyang.com/}{brandonyifanyang.com} \quad \faEnvelope \, \href{mailto:branyang@virginia.edu}{branyang@virginia.edu}}

\newcommand{\school}{Northwestern University}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[
  breaklinks,
  pdftitle={\yourname - \soptitle},
  pdfauthor={\yourname},
  unicode,
  colorlinks,
]{hyperref}

\begin{document}

\noindent In the modern era of embodied Artificial Intelligence (AI) systems, the integration of Natural Language Processing (NLP) and Computer Vision (CV) foundational models trained on extensive data has propelled efforts to develop generalist robotic policies. Despite these efforts, current models often fall short in \textbf{reasoning}, \textbf{planning}, and \textbf{executing} \textit{long-horizon} tasks. My research focuses on addressing these challenges at the intersection of NLP and robotics, where I aim to:
\begin{enumerate}[label=(\arabic*), itemindent=0pt, itemsep=0pt, parsep=0pt, nosep]
  \item \textbf{Develop embodied agents that follow and interact with natural language \textit{collaboratively}.}
  \item \textbf{Leverage language for reasoning and planning to navigate complex environments and solve long-horizon tasks.}
\end{enumerate}

\statement{Reinforcement Learning and the Need for Reasoning} Robotics is inherently collaborative, yet enabling true \textit{autonomous} cooperation in \textit{multi-agent} systems remains challenging. I joined the \href{https://www.collabrobotics.com/}{Collaborative Robotics Lab} at the \textbf{University of Virginia} (UVA) with \href{https://www.tiqbal.com/}{Prof. Tariq Iqbal}, focusing on multi-agent RL to develop collaborative policies. I developed simulation environments for complex assembly tasks and designed offline centralized RL policies. However, I realized our robots' successes depended on reward functions requiring labor-intensive design processes. This highlighted a significant limitation: \textit{the inability of robots to adapt to new environments without extensive human input}. I became curious about whether robots could mimic how humans navigate complex environments through innate reasoning rather than external supervision.

\statement{Language and Vision Guided Robotic Manipulation} Motivated by the limitations of reward supervision, I explored \textit{language} and \textit{vision}-driven approaches to enable autonomous reasoning and action. I developed \textit{GLOMA: Grounded Location for Object Manipulation}, a framework using large language models (LLM) and image diffusion models to generate goal images from language instructions. \textit{GLOMA} enables robots to execute goal-conditioned policies without the need for manually crafted reward functions, allowing them to autonomously imagine subgoals for long-horizon tasks. I led the development of \textit{GLOMA}, including dataset creation and model fine-tuning. However, I realized that 2D image-based methods fail to capture the 3D semantics of natural environments, motivating me to explore 3D-based methods for robotic perception.

\statement{Advancing Robotic Perception with 3D Gaussian Splatting} To continue goal synthesis motivated by autonomous policies, I expanded to 3D by collaborating with \href{https://jbhuang0604.github.io/}{Prof. Jia-Bin Huang} at the \textbf{University of Maryland} and colleagues from \textbf{MIT}. Our research addresses the limitations of 2D synthesis in environments needing 3D understanding, such as vertical displacement. We leverage 3D Gaussian Splatting (3DGS) for highly accurate 3D field representation, enabling more effective robotic perception. To enhance semantic understanding, we inject embeddings from large 2D foundational models into 3DGS, allowing robots to comprehend scene semantics and perform object-level edits. Our preliminary results demonstrate that 3D goal synthesis enables more robust and precise robotic manipulation. As project lead, I developed the codebase and explored various embedding injection techniques.

\statement{Enhancing Robotic Planning with Skill-Conditioned Architectures} While goal-synthesis methods in 2D and 3D can be robust and interpretable, they incur significant computational overhead due to their multi-step processes. Recent research has focused on end-to-end Vision-Language-Action (VLA) models to streamline this process. However, these models lack interpretability and generalization, especially with out-of-distribution data. To overcome these limitations, I am collaborating with \href{https://yenlingkuo.com/}{Prof. Yen-Ling Kuo} at \textbf{UVA} to develop \textit{SkillVLA}, a novel VLA model that enhances long-horizon, language-guided robotic policies by introducing a \textit{skill-conditioned} action output space. In \textit{SkillVLA}, each action is grounded to a specific skill—such as \textit{grasp} or \textit{lift}—which improves the interpretability of the policy. This enables robots to perform complex tasks more efficiently and adapt to diverse environments. I plan to submit this work to \textbf{RSS 2025} and believe \textit{SkillVLA} can advance skill-based learning in modern robotic manipulations.

\statement{Future Plans} I plan to extend my research in skill-conditioned reasoning and planning for embodied agents by developing systems that utilize \textbf{complex semantic concepts} (e.g., \textit{object affordances, spatial relations, grasping strategies}) to enhance their understanding of the physical world. Acquiring these concepts alongside skill manipulation priors (e.g., \textit{picking, placing}) enables embodied agents to \textbf{reason}, \textbf{plan}, and \textbf{execute} actions based on a deeper understanding of objects and scenes. This approach could facilitate \textbf{robust generalist robotic policies} for language-guided manipulation in complex environments. I will leverage insights from NLP and multimodal communities, which have made significant progress in modeling semantic structures across perceptual inputs. Furthermore, I believe richer 3D environment representations can enhance concept learning, allowing agents more informed reasoning about the physical world.

\noindent I also aim to develop embodied agents that work \textbf{\textit{collaboratively}}. While modern LLMs can converse with humans, current robot systems have yet to fully leverage this capability for collaboration. By integrating NLP methods for collaborative dialogue into embodied AI, I aim to enable robots to perform \textit{collaborative} tasks, such as assembling furniture or cleaning, in partnership with humans or other agents.

\statement{Why Northwestern} The M.S. Computer Science program at Northwestern provides an ideal environment to pursue my academic and research goals. Its thesis option and emphasis on research-driven learning perfectly align with my aspirations to develop intelligent embodied agents. I am particularly excited about courses such as \textit{Machine Learning and Artificial Intelligence for Robotics} and \textit{Deep Learning for Natural Language Processing}, which will provide the necessary foundation for designing collaborative agents capable of multimodal reasoning and long-horizon planning. Additionally, I look forward to exploring coursework in computer vision and 3D perception as they are crucial for developing robust robotic manipulation systems.

\noindent In addition to the curriculum, I am excited about the opportunity to work with faculty members whose research aligns with my interests. I am eager to collaborate with \href{https://manlingli.github.io/}{\textbf{Prof. Manling Li}}, whose work on multimodal interactions and compositional reasoning matches closely with my research goals. Her focus on trustworthy AI resonates with my interest in enhancing embodied agents through structured language-guided learning. I am also drawn to \href{https://argallab.northwestern.edu/}{\textbf{Prof. Brenna Argall}}'s research on assistive robotics and shared autonomy, particularly her innovative Body Machine Interface for improving human-robot interaction. Additionally, I am inspired by \href{https://zhaoranwang.github.io/}{\textbf{Prof. Zhaoran Wang}}'s work on efficient reinforcement learning, which complements my interest in scalable and cooperative multi-agent systems.

\noindent Northwestern's collaborative and interdisciplinary environment, along with its distinguished faculty and resources, makes it the ideal place to advance my research aspirations. I am confident that this M.S. program will provide the strong academic foundation and research experience necessary to prepare me for a future Ph.D., where I hope to further explore innovative solutions in robotics and AI.

\end{document}