In the modern era of embodied Artificial Intelligence (AI) systems, the integration of Natural Language Processing (NLP) and Computer Vision (CV) foundational models trained on extensive data has propelled efforts to develop generalist robotic policies. These advancements have enabled intelligent agents to recognize patterns and perform manipulative tasks with increasing proficiency. However, despite these efforts, current models often fall short in reasoning, planning, and executing long-horizon tasks. The challenges of autonomously generating strategies, employing hierarchical reasoning, and making informed decisions remain unresolved in embodied AI. 

My research interests are driven by these challenges and focus on the intersection of NLP and robotics, where I aim to:
(1)	Develop embodied agents that follow and interact with natural language collaboratively.
(2)	Leverage language for reasoning and planning to navigate complex environments and solve long-horizon tasks.

My interests are shaped by my past research experiences, which involved Reinforcement Learning (RL) for robotics, language and vision-guided manipulation, 3D perception, and natural language reasoning.

Robotics is inherently collaborative, yet enabling true autonomous cooperation in multi-agent systems remains a significant challenge. To pursue my interest in collaborative agents, I joined the Collaborative Robotics Lab at the University of Virginia (UVA) with Prof. Tariq Iqbal, focusing on researching multi-agent RL to develop collaborative robotic policies. I developed simulation environments for complex assembly tasks and designed offline centralized RL policies that enabled robots to collaborate effectively. However, I realized that our robots’ successes heavily depended on meticulously crafted reward functions, which required labor-intensive and supervised design processes. This reliance highlighted a significant limitation: the inability of robots to adapt to new environments without extensive human input. Observing this, I became curious about whether robots could mimic how humans navigate complex environments through innate reasoning rather than external supervision.

Motivated by the limitations of reward supervision, I explored language and vision-driven approaches to enable autonomous reasoning and action. Supported by the Dean’s Engineering Research Scholarship at UVA, I developed GLOMA: Grounded Location for Object Manipulation, a framework using large language models (LLM) and image diffusion models to generate goal images from language instructions. GLOMA enables robots to execute goal-conditioned policies without manually crafted reward functions, thereby autonomously imagine subgoals for long-horizon tasks. I led the development of GLOMA, including dataset creation and model fine-tuning, and presented it at multiple research symposiums. However, as robotics scenarios grow more complex, I realized that traditional 2D image-based methods fail to capture the 3D semantics and object relationships of natural environments, motivating me to explore 3D-based methods for fine-grained robotic perception.

To continue goal synthesis motivated by autonomous robotic policies, I expanded this capability to 3D by collaborating with Prof. Jia-Bin Huang at the University of Maryland and colleagues from MIT. Our research addresses the limitations of 2D synthesis in environments needing 3D understanding, such as vertical displacement. We leverage 3D Gaussian Splatting (3DGS) for highly accurate 3D field representation, enabling more effective robotic perception. To enhance semantic understanding, we inject embeddings from large 2D foundational models into 3DGS, allowing robots to comprehend scene semantics and perform object-level edits. Our preliminary results demonstrate that 3D goal synthesis enables more robust and precise robotic manipulation. As project lead, I developed the codebase and explored various embedding injection techniques to achieve this enhanced performance.

While goal-synthesis methods in both 2D and 3D are robust and interpretable, they often incur significant computational overhead due to their complex, multi-step processing pipelines. Recent robotics research has focused on developing end-to-end Vision-Language-Action (VLA) models to streamline this process. However, these models often lack interpretability and struggle with generalization, especially when faced with out-of-distribution data. To overcome these limitations, I am collaborating with Prof. Yen-Ling Kuo at UVA to develop SkillVLA, a novel architecture that enhances long-horizon, language-guided robotic policies by introducing a skill-conditioned action output space. In SkillVLA, each action is grounded to a specific skill—such as grasp or lift—which improves both the interpretability and robustness of the policy. This structured approach enables robots to perform complex tasks more efficiently and adapt to diverse environments. As we prepare to submit this work to RSS 2025, I am excited about the potential of SkillVLA to inspire a new direction in skill-based learning for modern robotic manipulation systems.

I plan to extend my research in skill-conditioned reasoning and planning for embodied agents by developing systems that utilize complex semantic concepts (e.g., object affordances, spatial relations, grasping strategies, object interaction strategies) to enhance their understanding of the physical world. In addition to acquiring useful skill manipulation priors (e.g., picking, grasping, placing), learning complex semantic concepts enables embodied agents to reason about the objects and scenes they interact with, and to plan and execute their actions based on this understanding. This approach could facilitate the development of robust generalist robotic policies capable of performing language-guided manipulation in complex scenes. I will leverage insights from NLP and multimodal communities, which have made significant progress in modeling semantic structures across various perceptual inputs. Furthermore, building on my work with 3DGS, I believe richer 3D environment representations can enhance concept learning, allowing agents more informed reasoning about the physical world.

Drawing from my experience for developing collaborative agents, I also aim to develop embodied agents that can reason, plan, and execute tasks collaboratively . While modern LLMs can converse with humans, current robot systems have yet to fully leverage this capability for collaboration in physical tasks. By integrating NLP methods for collaborative dialogue into embodied AI, I aim to enable robots to perform collaborative tasks, such as assembling furniture or cleaning, in partnership with humans or other agents.

The M.S. Computer Science at UCI provides the strong academic rigor and diverse research community necessary for my pursuit of a future Ph.D. in embodied AI. To strengthen my technical background in computer vision, I am excited about taking courses such as Advanced 3D Computer Graphics. I am also eager to take UCI’s project-based courses, including Project in Artificial Intelligence. The hands-on nature of these courses suits my personal learning style and will enable me to enrich my foundational knowledge of AI.

In addition to the rigorous coursework offered at UCI, I am particularly drawn to the opportunity to conduct research in the M.S. CS Thesis Plan. I am very interested in collaborating with Prof. Unnat Jain, whose focus on language-guided robot learning and self-supervised learning aligns closely with my own research goals in NLP and robotics. Additionally, I am excited to explore potential collaborations with Prof. Roy Fox, whose work in imitation learning, skill learning, and RL complements my research background and future ambitions. Additionally, I am drawn to the research of Prof. Sameer Singh, whose lab conducts reasoning research in NLP. Their work inspires me to use these concepts to improve reasoning capabilities in embodied agents. Lastly, I am excited to engage with the larger ML community at UCI, such as the Center for Machine Learning and Intelligent Systems, where there are diverse and interdisciplinary research projects that I can both contribute to and learn from.