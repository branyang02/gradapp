\documentclass[12pt]{article}
\usepackage[
  a4paper,
  margin=1in,
  headsep=18pt, % separation between header rule and text
]{geometry}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{microtype}
\usepackage[backend=biber,style=numeric]{biblatex}
\usepackage{setspace}
% \usepackage{mathpazo}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{fontawesome}


\definecolor{customred}{HTML}{d14836}


\addbibresource{references.bib}

\singlespacing
% \setstretch{1.5}
\setlength{\parskip}{1em} 

\newcommand{\statement}[1]{\medskip\noindent
  \textcolor{black}{\textbf{#1}}\space
}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{
\textsc{\soptitle}\hfill\footnotesize
\textbf{\yourname}\\
\school\hfill\yourintent}
\fancyfoot[C]{\footnotesize\thepage}

\let\oldcenter\center
\let\oldendcenter\endcenter
\renewenvironment{center}{\setlength\topsep{0pt}\oldcenter}{\oldendcenter}

\newcommand{\wordcount}{
\begin{center}
\rule{0.75\textwidth}{.4pt}\\
{\footnotesize A statement of \numwords \ words}
\end{center}
}

\newif\ifcomments

\newcommand{\comment}[1]{}
\newcommand{\todo}[1]{\ifcomments\textcolor{red}{TODO: #1}\fi}

%%%%%%%%%%%%%%%%%%%%% Edit this section %%%%%%%%%%%%%%%%%%%%

\commentstrue

\newcommand{\soptitle}{Statement of Purpose}
\newcommand{\yourname}{Brandon Y. Yang}
\newcommand{\yourintent}{\faLink \, \href{https://brandonyifanyang.com/}{brandonyifanyang.com} \quad \faEnvelope \, \href{mailto:branyang@virginia.edu}{branyang@virginia.edu}}

\newcommand{\school}{UCSD M.S. Computer Science}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[
  breaklinks,
  pdftitle={\yourname - \soptitle},
  pdfauthor={\yourname},
  unicode,
  colorlinks,
]{hyperref}

\begin{document}

\noindent In the modern era of embodied Artificial Intelligence (AI) systems, the integration of Natural Language Processing (NLP) and Computer Vision (CV) foundational models trained on extensive data has propelled efforts to develop generalist robotic policies. Despite these efforts, current models often fall short in \textbf{reasoning}, \textbf{planning}, and \textbf{executing} \textit{long-horizon} tasks. My research focuses on addressing these challenges at the intersection of NLP and robotics, where I aim to:
\begin{enumerate}[label=(\arabic*), itemindent=0pt, itemsep=0pt, parsep=0pt, nosep]
    \item \textbf{Develop embodied agents that follow and interact with natural language \textit{collaboratively}.}
    \item \textbf{Leverage language for reasoning and planning to navigate complex environments and solve long-horizon tasks.}
\end{enumerate}

\statement{Reinforcement Learning and the Need for Reasoning} Robotics is inherently collaborative, yet enabling true \textit{autonomous} cooperation in \textit{multi-agent} systems remains challenging. I joined the \href{https://www.collabrobotics.com/}{Collaborative Robotics Lab} at the \textbf{University of Virginia} (UVA) with \href{https://www.tiqbal.com/}{Prof. Tariq Iqbal}, focusing on multi-agent RL to develop collaborative policies. I developed simulation environments for complex assembly tasks and designed offline centralized RL policies. However, I realized our robots' successes depended on reward functions requiring labor-intensive design processes. This highlighted a significant limitation: \textit{the inability of robots to adapt to new environments without extensive human input}. I became curious about whether robots could mimic how humans navigate complex environments through innate reasoning rather than external supervision.

\statement{Language and Vision Guided Robotic Manipulation} Motivated by the limitations of reward supervision, I explored \textit{language} and \textit{vision}-driven approaches to enable autonomous reasoning and action. I developed \textit{GLOMA: Grounded Location for Object Manipulation}, a framework using large language models (LLM) and image diffusion models to generate goal images from language instructions. \textit{GLOMA} enables robots to execute goal-conditioned policies without the need for manually crafted reward functions, allowing them to autonomously imagine subgoals for long-horizon tasks. I led the development of \textit{GLOMA}, including dataset creation and model fine-tuning. However, I realized that 2D image-based methods fail to capture the 3D semantics of natural environments, motivating me to explore 3D-based methods for robotic perception.

\statement{Advancing Robotic Perception with 3D Gaussian Splatting} To continue goal synthesis motivated by autonomous policies, I expanded to 3D by collaborating with \href{https://jbhuang0604.github.io/}{Prof. Jia-Bin Huang} at the \textbf{University of Maryland} and colleagues from \textbf{MIT}. Our research addresses the limitations of 2D synthesis in environments needing 3D understanding, such as vertical displacement. We leverage 3D Gaussian Splatting (3DGS) for highly accurate 3D field representation, enabling more effective robotic perception. To enhance semantic understanding, we inject embeddings from large 2D foundational models into 3DGS, allowing robots to comprehend scene semantics and perform object-level edits. Our preliminary results demonstrate that 3D goal synthesis enables more robust and precise robotic manipulation. As project lead, I developed the codebase and explored various embedding injection techniques.

\statement{Enhancing Robotic Pla    nning with Skill-Conditioned Architectures} While goal-synthesis methods in 2D and 3D can be robust and interpretable, they incur significant computational overhead due to their multi-step processes. Recent research has focused on end-to-end Vision-Language-Action (VLA) models to streamline this process. However, these models lack interpretability and generalization, especially with out-of-distribution data. To overcome these limitations, I am collaborating with \href{https://yenlingkuo.com/}{Prof. Yen-Ling Kuo} at \textbf{UVA} to develop \textit{SkillVLA}, a novel VLA model that enhances long-horizon, language-guided robotic policies by introducing a \textit{skill-conditioned} action output space. In \textit{SkillVLA}, each action is grounded to a specific skill—such as \textit{grasp} or \textit{lift}—which improves the interpretability of the policy. This enables robots to perform complex tasks more efficiently and adapt to diverse environments. I plan to submit this work to \textbf{RSS 2025} and believe \textit{SkillVLA} can advance skill-based learning in modern robotic manipulations.

\statement{Future Plans} I plan to extend my research in skill-conditioned reasoning and planning for embodied agents by developing systems that utilize \textbf{complex semantic concepts} (e.g., \textit{object affordances, spatial relations, grasping strategies}) to enhance their understanding of the physical world. Acquiring these concepts alongside skill manipulation priors (e.g., \textit{picking, placing}) enables embodied agents to \textbf{reason}, \textbf{plan}, and \textbf{execute} actions based on a deeper understanding of objects and scenes. This approach could facilitate \textbf{robust generalist robotic policies} for language-guided manipulation in complex environments. I will leverage insights from NLP and multimodal communities, which have made significant progress in modeling semantic structures across perceptual inputs. Furthermore, I believe richer 3D environment representations can enhance concept learning, allowing agents more informed reasoning about the physical world.

\noindent I also aim to develop embodied agents that work \textbf{\textit{collaboratively}}. While modern LLMs can converse with humans, current robot systems have yet to fully leverage this capability for collaboration. By integrating NLP methods for collaborative dialogue into embodied AI, I aim to enable robots to perform \textit{collaborative} tasks, such as assembling furniture or cleaning, in partnership with humans or other agents.

\statement{Why UCSD} The M.S. Computer Science at UCSD provides the strong academic rigor and diverse research community necessary for my pursuit of a future Ph.D. in embodied AI. There are several faculty members at UCSD whose research aligns with my interests. First, Prof. Hao Su's work on robot learning has inspired my research in robotics, and I am excited to collaborate with him on developing agents that can reason through concepts. In addition, Prof. Su also places an emphasis on simplifying overall robotics overheads, and he created Maniskill, which encourages further research in robot learning. I beleive that robot simulation can drive up scaling, allowing robot foundational models to achieve or surpass the capabilities of LLMs. I am also excited to have a chance to work with Prof. Xiaolong Wang. His unique approach of leveraging 3D to better represent the robotic workspace deeply resonates with my experiences and goals. I am very excited about robotics in 3D, and I am also interested in extending it to perform reasoning using language in the 3D space for manipulation tasks. I am confident that my research experience and future goals align well with the research interests of the faculty at UCSD, and I am excited to contribute to the vibrant research community at UCSD.


\end{document}