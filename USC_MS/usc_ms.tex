\documentclass[10pt]{article}
\usepackage[
  a4paper,
  margin=1in,
  headsep=18pt, % separation between header rule and text
]{geometry}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{microtype}
\usepackage[backend=biber,style=numeric]{biblatex}
\usepackage{setspace}
% \usepackage{mathpazo}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{fontawesome}


\definecolor{customred}{HTML}{d14836}


\addbibresource{references.bib}

\singlespacing
% \setstretch{1.5}
\setlength{\parskip}{1em} 

\newcommand{\statement}[1]{\medskip\noindent
  \textcolor{black}{\textbf{#1}}\space
}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{
\textsc{\soptitle}\hfill\footnotesize
\textbf{\yourname}\\
\school\hfill\yourintent}
\fancyfoot[C]{\footnotesize\thepage}

\let\oldcenter\center
\let\oldendcenter\endcenter
\renewenvironment{center}{\setlength\topsep{0pt}\oldcenter}{\oldendcenter}

\newcommand{\wordcount}{
\begin{center}
\rule{0.75\textwidth}{.4pt}\\
{\footnotesize A statement of \numwords \ words}
\end{center}
}

\newif\ifcomments

\newcommand{\comment}[1]{}
\newcommand{\todo}[1]{\ifcomments\textcolor{red}{TODO: #1}\fi}

%%%%%%%%%%%%%%%%%%%%% Edit this section %%%%%%%%%%%%%%%%%%%%

\commentstrue

\newcommand{\soptitle}{Personal Statement}
\newcommand{\yourname}{Brandon Y. Yang}
\newcommand{\yourintent}{\faLink \, \href{https://brandonyifanyang.com/}{brandonyifanyang.com} \quad \faEnvelope \, \href{mailto:branyang@virginia.edu}{branyang@virginia.edu}}

\newcommand{\school}{University of Southern California}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[
  breaklinks,
  pdftitle={\yourname - \soptitle},
  pdfauthor={\yourname},
  unicode,
  colorlinks,
]{hyperref}

\begin{document}


\noindent In the modern era of embodied Artificial Intelligence (AI) systems, the integration of Natural Language Processing (NLP) and Computer Vision (CV) foundational models trained on extensive data has propelled efforts to develop generalist robotic policies. These advancements have enabled intelligent agents to recognize patterns and perform manipulative tasks with increasing proficiency. However, despite these efforts, current models often fall short in \textbf{reasoning}, \textbf{planning}, and \textbf{executing} \textit{long-horizon} tasks. The challenges of autonomously generating strategies, employing hierarchical reasoning, and making informed decisions remain unresolved in embodied AI. My research interests are driven by these challenges and focus on the intersection of NLP and robotics, where I aim to:
\begin{enumerate}[label=(\arabic*), itemindent=0pt, itemsep=0pt, parsep=0pt, nosep]
  \item \textbf{Develop embodied agents that follow and interact with natural language \textit{collaboratively}.}
  \item \textbf{Leverage language for reasoning and planning to navigate complex environments and solve long-horizon tasks.}
\end{enumerate}
My interests are shaped by my past research experiences, which involved Reinforcement Learning (RL) for robotics, language and vision-guided manipulation, 3D perception, and natural language reasoning.

\statement{Reinforcement Learning and the Need for Reasoning} Robotics is inherently collaborative, yet enabling true \textit{autonomous} cooperation in \textit{multi-agent} systems remains a significant challenge. To pursue my interest in collaborative agents, I joined the \href{https://www.collabrobotics.com/}{Collaborative Robotics Lab} at the \textbf{University of Virginia} (UVA) with  \href{https://www.tiqbal.com/}{Prof. Tariq Iqbal}, focusing on researching multi-agent RL to develop collaborative robotic policies. I developed simulation environments for complex assembly tasks and designed offline centralized RL policies that enabled robots to collaborate effectively. However, I realized that our robots' successes heavily depended on meticulously crafted reward functions, which required labor-intensive and supervised design processes. This reliance highlighted a significant limitation: \textit{the inability of robots to adapt to new environments without extensive human input}. Observing this, I became curious about how robots could mimic the way humans navigate complex environments through innate reasoning rather than external supervision.

\statement{Language and Vision Guided Robotic Manipulation} Motivated by the limitations of reward supervision in RL policies, I explored \textit{language} and \textit{vision}-driven approaches to enable robots to \textit{reason} and \textit{act} autonomously. Supported by the Dean's Engineering Research Scholarship at UVA, I collaborated with colleagues at the Collaborative Robotics Lab to develop \textit{GLOMA: Grounded Location for Object Manipulation}, a novel framework that leverages LLMs and image diffusion models to generate goal images for robotic manipulation tasks based on language instructions. \textit{GLOMA} first uses an LLM to predict rearranged object bounding boxes in a scene based on a language prompt, and then employs a diffusion model to generate the new scene. This allows robots to execute goal-conditioned policies—such as RL and Behavioral Cloning (BC) — without manually crafted reward functions, enabling them to imagine subgoals for complex tasks. As the project leader, I led the development of the \textit{GLOMA} model, from conceptualization to implementation, including creating the manipulation dataset and fine-tuning the base language model to enhance its reasoning capabilities. However, recognizing the limitations of 2D methods in capturing the full 3D semantics of complex environments, I sought to explore 3D-based methods for robotic perception.

\statement{Advancing Robotic Perception with 3D Gaussian Splatting} To continue goal synthesis motivated by autonomous robotic policies, I expanded this capability to 3D by collaborating with \href{https://jbhuang0604.github.io/}{Prof. Jia-Bin Huang} at the \textbf{University of Maryland} and colleagues from \textbf{MIT}. Our ongoing research addresses the limitations of 2D image-based goal synthesis, which falls short in environments requiring 3D understanding, such as scenarios involving vertical displacement that cannot be captured in 2D. We leverage 3D Gaussian Splatting (3DGS) for highly accurate 3D field representation, enabling more effective robotic perception. To enhance semantic understanding, we inject embeddings from large 2D foundational models into 3DGS, allowing robots to comprehend scene semantics and perform object-level edits. This process is supported by video segmentation to maintain temporal consistency and ensure reliable integration of 2D training data into 3D scenes. Preliminary findings suggest that 3D goal synthesis with 3DGS can lead to more robust robotic policies due to the enhanced environmental understanding. I led the development of this research, experimenting with various embedding injection techniques. I am eager to further explore the potential of this approach for robotic perception and manipulation.

\statement{Enhancing Robotic Planning with Skill-Conditioned Architectures} While goal-synthesis methods in both 2D and 3D are robust and interpretable, they often incur significant computational overhead due to their complex, multi-step processing pipelines. Recent robotics research has focused on developing end-to-end Vision-Language-Action (VLA) models to streamline this process. However, these models often lack interpretability and struggle with generalization, especially when faced with out-of-distribution data. To overcome these limitations, I am collaborating with \href{https://yenlingkuo.com/}{Prof. Yen-Ling Kuo} at \textbf{UVA} to develop \textit{SkillVLA}, a novel architecture that enhances long-horizon, language-guided robotic policies by introducing a \textit{skill-conditioned} action output space. In \textit{SkillVLA}, each action is grounded to a specific skill—such as \textit{grasp} or \textit{lift}—which improves both the interpretability and robustness of the policy. This structured approach enables robots to perform complex tasks more efficiently and adapt to diverse environments. As we prepare to submit this work to \textbf{RSS 2025}, I am excited about the potential of \textit{SkillVLA} to inspire a new direction in skill-based learning for modern robotic manipulation systems.

\statement{Future Plans} I plan to extend my research in skill-conditioned reasoning and planning for embodied agents by developing systems that utilize \textbf{complex semantic concepts} (e.g., \textit{object affordances, spatial relations, grasping strategies, object interaction strategies}) to enhance their understanding of the physical world. In addition to acquiring useful skill manipulation priors (e.g., \textit{picking, grasping, placing}), learning complex semantic concepts enables embodied agents to \textbf{reason} about the objects and scenes they interact with, and to \textbf{plan} and \textbf{execute} their actions based on this understanding. This approach could facilitate the development of \textbf{robust generalist robotic policies} capable of performing language-guided manipulation in complex scenes. I will leverage insights from NLP and multimodal communities, which have made significant progress in modeling semantic structures across various perceptual inputs. In addition, building on my prior work with 3DGS, I believe richer 3D environment representations can further enhance concept learning, as they provide a detailed understanding of the physical world, allowing agents to have more informed reasoning about the objects and scenes they interact with.

\noindent Building on my experience and passion for collaborative agents, I also aim to develop embodied agents that can \textbf{reason, plan, and execute tasks \textit{collaboratively}}. While modern foundational LLM systems are capable of conversing with humans, current robotic systems have yet to fully leverage this capability for collaboration, either with other agents or with humans in the physical world. By developing agents that can reason and plan based on natural language instructions, we can enable robots to perform \textit{collaborative} tasks, such as assembling furniture, cooking, or cleaning. To achieve this, I plan to integrate existing NLP methods for collaborative dialogue into embodied AI, combining them with robotic manipulation systems to enable collaborative task execution.

\statement{Why USC M.S.} The M.S. Computer Science program at USC provides the strong academic rigor and diverse research community necessary for my pursuit of a future Ph.D. in embodied AI. To strengthen my technical background in computer vision, I am excited about taking courses such as \textit{CSCI 580: 3-D Graphics and Rendering} and \textit{CSCI 677: Advanced Computer Vision}. These courses will offer valuable insights into current practices in 3D computer vision applications and their integration into robotic systems. In addition, I want to deepen my understanding of the latest advancements in NLP, particularly in aligning large language models (LLMs) during post-training for deployment in downstream applications, including smart AI agents and robotics. Courses such as \textit{CSCI 544: Applied Natural Language Processing} and \textit{CSCI 662: Advanced Natural Language Processing} will equip me with the knowledge to achieve these goals. Lastly, I also aim to strengthen my foundational skills in robotics, focusing on classical methods like joint control and motion planning. USC's course offerings such as \textit{CSCI 547: Robot Dynamics and Control}, will help me build a strong understanding of these essential principles.

\noindent In addition to the rigorous courwork offered by the program, I am also excited about the opportunity to conduct research in the Thesis Track. I am particularly drawn to the research of \href{https://ebiyik.github.io/}{\textbf{Prof. Erdem Bıyık}}, whose work in human-robot interaction and multi-agent systems strongly aligns with my experience and aspirations.  His focus on vision and language learning for robotic policies, as demonstrated in projects like \textit{RL-VLM-F} and \textit{Trajectory Improvement and Reward Learning from Comparative Language Feedback}, resonates deeply with my own interests. I am also eager to explore potential collaborations with \href{https://danielseita.github.io/}{\textbf{Prof. Daniel Seita}}, whose research on multimodal observation and action representations, and human-robot interaction complements my research goals. Additionally, I am excited about the opportunity to engage with the larger robotics  community at USC, such as the Robotics and Autonomous Systems Center (RASC), wehre there are diverse and interdisplicinary research projects that I can contribute to and learn from.

\end{document}